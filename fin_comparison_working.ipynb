{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import imutils\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "#sys.path.append(os.path.dirname(__file__) + \"/../\")\n",
    "from scipy.misc import imread, imsave\n",
    "from skimage.measure import structural_similarity as ssim\n",
    "from config import load_config\n",
    "from dataset.factory import create as create_dataset\n",
    "from nnet import predict\n",
    "from util import visualize\n",
    "import cv2\n",
    "from dataset.pose_dataset import data_to_input\n",
    "\n",
    "\n",
    "from multiperson.detections import extract_detections\n",
    "from multiperson.predict import SpatialModel, eval_graph, get_person_conf_multicut\n",
    "from multiperson.visualize import PersonDraw, visualize_detections\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tkinter import messagebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vibrate():\n",
    "    pass\n",
    "def compare_images(slope1, slope2, allowance):\n",
    "    for key in slope1:\n",
    "        if abs(slope1[key]-slope2[key]) > allowance:\n",
    "            vibrate(key)\n",
    "            print(\"error at : \", key)\n",
    "            \n",
    "            \n",
    "def slope_calc(co1):\n",
    "    body_dict={'nose_right': co1[0],\n",
    "               'nose_left': co1[1],\n",
    "              'right_eye_ear': co1[2],\n",
    "              'left_eye_ear': co1[3],\n",
    "              'right_upper_arm':co1[4],\n",
    "              'left_upper_arm':co1[5],\n",
    "              'right_forearm': co1[6],\n",
    "              'left_forearm': co1[7],\n",
    "               'right_upper_leg':co1[8],\n",
    "               'left_upper_leg':co1[9],\n",
    "               'right_shin':co1[10],\n",
    "               'left_shin':co1[11]\n",
    "              }\n",
    "    body_dict['backbone']=[(int((body_dict['right_upper_arm'][0][0]+body_dict['left_upper_arm'][0][0])/2),\n",
    "                           int((body_dict['right_upper_arm'][0][1]+body_dict['left_upper_arm'][0][1])/2)),\n",
    "                           (int((body_dict['right_upper_leg'][0][0]+body_dict['left_upper_leg'][0][0])/2),\n",
    "                           int((body_dict['right_upper_leg'][0][1]+body_dict['left_upper_leg'][0][1])/2))]\n",
    "    for key in body_dict:\n",
    "        a=math.atan((body_dict['backbone'][1][1]-body_dict['backbone'][0][1])/(body_dict['backbone'][0][0]-body_dict['backbone'][1][0]))\n",
    "        slopes[key]=(math.atan((body_dict[key][1][1]-body_dict[key][0][1])/(body_dict[key][0][0]-body_dict[key][1][0])))-a\n",
    "    return slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict(frame):\n",
    "\n",
    "    # Load and setup CNN part detector\n",
    "    tf.reset_default_graph()\n",
    "    image= frame\n",
    "    image_batch = data_to_input(frame)\n",
    "    # Compute prediction_n with the CNN\n",
    "    outputs_np = sess.run(outputs, feed_dict={inputs: image_batch})\n",
    "    scmap, locref, pairwise_diff = predict.extract_cnn_output(outputs_np, cfg, dataset.pairwise_stats)\n",
    "    detections = extract_detections(cfg, scmap, locref, pairwise_diff)\n",
    "    unLab, pos_array, unary_array, pwidx_array, pw_array = eval_graph(sm, detections)\n",
    "    m=time.time()\n",
    "    person_conf_multi = get_person_conf_multicut(sm, unLab, unary_array, pos_array)\n",
    "    img = np.copy(image)\n",
    "    #coor = PersonDraw.draw()\n",
    "    visim_multi = img.copy()\n",
    "    co1=draw_multi.draw2(visim_multi, dataset, person_conf_multi, image)\n",
    "    return co1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/coco/coco-resnet-101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/coco/coco-resnet-101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_people:  2\n",
      "#tracked objects: 0\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "num_people:  3\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "nose_right\n",
      "nose_left\n",
      "right_eye_ear\n",
      "left_eye_ear\n",
      "right_upper_arm\n",
      "left_upper_arm\n",
      "right_forearm\n",
      "left_forearm\n",
      "right_upper_leg\n",
      "left_upper_leg\n",
      "right_shin\n",
      "left_shin\n",
      "backbone\n",
      "num_people:  3\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "nose_right\n",
      "nose_left\n",
      "right_eye_ear\n",
      "left_eye_ear\n",
      "right_upper_arm\n",
      "left_upper_arm\n",
      "right_forearm\n",
      "left_forearm\n",
      "right_upper_leg\n",
      "left_upper_leg\n",
      "right_shin\n",
      "left_shin\n",
      "backbone\n",
      "num_people:  5\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "num_people:  5\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n",
      "num_people:  1\n",
      "#tracked objects: 1\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "cfg = load_config(\"demo/pose_cfg_multi.yaml\")\n",
    "dataset=create_dataset(cfg)\n",
    "sm = SpatialModel(cfg)\n",
    "sm.load()\n",
    "tf.reset_default_graph()\n",
    "draw_multi = PersonDraw()\n",
    "sess, inputs, outputs = predict.setup_pose_prediction(cfg)\n",
    "fps_time=0\n",
    "# Read image from file\n",
    "slopes={}\n",
    "dir=os.listdir(\"stick\")\n",
    "k=0\n",
    "cap=cv2.VideoCapture('exer.mp4')\n",
    "cap_user=cv2.VideoCapture(0)\n",
    "i=0\n",
    "while (True):\n",
    "    ret, orig_frame= cap.read()\n",
    "    ret2, orig_frame_user= cap_user.read()\n",
    "    if i%25 == 0:                   \n",
    "        \n",
    "        frame = cv2.resize(orig_frame, (0, 0), fx=0.50, fy=0.50)\n",
    "        #frame=orig_frame\n",
    "        user_frame=cv2.resize(orig_frame_user, (0, 0), fx=0.50, fy=0.50)\n",
    "        co1=run_predict(frame)\n",
    "        user_co1=run_predict(user_frame)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            slope_reqd=slope_calc(co1)\n",
    "            slope_user=slope_calc(user_co1)\n",
    "            compare_images(slope_reqd, slope_user)\n",
    "        except IndexError:\n",
    "            #if len(co1)!=len(user_co1):\n",
    "            messagebox.showinfo(\"Title\", \"Please adjust camera to show your keypoints\")\n",
    "            continue\n",
    "        cv2.putText(user_frame,\n",
    "                    \"FPS: %f\" % (1.0 / (time.time() - fps_time)),\n",
    "                    (10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                    (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.imshow('user_frame', user_frame)\n",
    "        cv2.imshow('frame', frame)\n",
    "        fps_time=time.time()\n",
    "        #visualize.waitforbuttonpress()\n",
    "        if cv2.waitKey(10)==ord('q'):\n",
    "            break\n",
    "elapsed= time.time()-start_time\n",
    "#print(\"sse score : \", sse)\n",
    "cap.release()\n",
    "cap_user.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(304, 54), (311, 49)],\n",
       " [(304, 54), (302, 49)],\n",
       " [(311, 49), (326, 54)],\n",
       " [(339, 87), (351, 132)],\n",
       " [(298, 88), (293, 128)],\n",
       " [(351, 132), (364, 172)],\n",
       " [(293, 128), (286, 161)],\n",
       " [(334, 173), (334, 237)],\n",
       " [(304, 172), (310, 237)],\n",
       " [(334, 237), (345, 300)],\n",
       " [(310, 237), (320, 294)]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_co1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(303, 60), (309, 56)],\n",
       " [(303, 60), (302, 54)],\n",
       " [(309, 56), (324, 60)],\n",
       " [(339, 96), (346, 142)],\n",
       " [(295, 95), (296, 136)],\n",
       " [(346, 142), (351, 180)],\n",
       " [(296, 136), (293, 173)],\n",
       " [(334, 181), (326, 243)],\n",
       " [(305, 181), (307, 242)],\n",
       " [(326, 243), (343, 302)],\n",
       " [(307, 242), (326, 298)]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.2) /io/opencv/modules/highgui/src/window.cpp:356: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cba546c2e5a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user_frame'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'frame'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.2) /io/opencv/modules/highgui/src/window.cpp:356: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture('exer.mp4')\n",
    "cap_user=cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame1=cap.read()\n",
    "    ret2, frame2=cap_user.read()\n",
    "    \n",
    "    cv2.imshow('user_frame', frame2)\n",
    "    cv2.imshow('frame', frame1)\n",
    "    if cv2.waitKey(10)==ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "cap_user.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "cap_user.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
